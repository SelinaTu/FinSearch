# -*- coding: utf-8 -*-
"""reward_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5t3G3n1GUzD0qt9Jc8UlLXzDNgfqz64
"""

# =============================
# PART 1: Train the Reward Model and Save It (with extended epochs & tuning)
# =============================

import torch
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import LoraConfig, get_peft_model
from trl import RewardTrainer, RewardConfig
from torch.utils.data import DataLoader

# 1) Load CSV
csv_path = "/scratch/wt2244/final_feedback20250430.csv"
for enc in ("utf-8", "latin1", "utf-8-sig"):
    try:
        df = pd.read_csv(csv_path, encoding=enc)
        print(f"âœ… Loaded CSV with encoding={enc}, shape={df.shape}")
        break
    except Exception as e:
        print(f"âœ– Failed loading with encoding={enc}: {e}")
else:
    raise RuntimeError(f"Could not load `{csv_path}` with any tried encodings")

# 2) Normalize column names
df.columns = [col.strip().lower() for col in df.columns]

# 3) Convert to Dataset & split
raw = Dataset.from_pandas(df, preserve_index=False)
splits = raw.train_test_split(test_size=0.10, seed=42)
train_pairs, eval_pairs = splits["train"], splits["test"]

# 4) Tokenizer & model setup
reward_model_name = "allenai/longformer-base-4096"
reward_tokenizer  = AutoTokenizer.from_pretrained(reward_model_name, use_fast=True)
if reward_tokenizer.pad_token is None:
    reward_tokenizer.pad_token = reward_tokenizer.eos_token or reward_tokenizer.sep_token

reward_model = AutoModelForSequenceClassification.from_pretrained(
    reward_model_name,
    num_labels=1
)
if reward_model.config.pad_token_id is None:
    reward_model.config.pad_token_id = reward_tokenizer.pad_token_id

# â€” enable gradient checkpointing for large models
reward_model.gradient_checkpointing_enable()

# 5) Apply LoRA
reward_lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["query", "value"],
    task_type="SEQ_CLS"
)
reward_model = get_peft_model(reward_model, reward_lora_config)

# 6) Prepare data for reward
def format_for_reward(ex):
    return {
        "chosen":   ex["prompt"] + "\n\nSummary: " + ex["chosen"],
        "rejected": ex["prompt"] + "\n\nSummary: " + ex["rejected"]
    }

train_data = train_pairs.map(format_for_reward, remove_columns=["prompt","chosen","rejected"])
eval_data  = eval_pairs.map(format_for_reward, remove_columns=["prompt","chosen","rejected"])

def tokenize_batch(batch):
    c = reward_tokenizer(batch["chosen"], truncation=True, padding="max_length", max_length=4096)
    r = reward_tokenizer(batch["rejected"], truncation=True, padding="max_length", max_length=4096)
    return {
        "input_ids_chosen":       c["input_ids"],
        "attention_mask_chosen":  c["attention_mask"],
        "input_ids_rejected":     r["input_ids"],
        "attention_mask_rejected":r["attention_mask"],
    }

train_data = train_data.map(tokenize_batch, batched=True)
eval_data  = eval_data.map(tokenize_batch, batched=True)

# 7) Training configuration (tuned)
reward_training_args = RewardConfig(
    num_train_epochs=23,           # â† increased from 5 to 11
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=20,
    logging_strategy="steps",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    output_dir="/scratch/wt2244/reward_model_longformer0502_tuned2"
)

reward_trainer = RewardTrainer(
    model=reward_model,
    args=reward_training_args,
    train_dataset=train_data,
    eval_dataset=eval_data,
    processing_class=reward_tokenizer,
    peft_config=reward_lora_config
)

# 8) Train
print("ðŸš€ Starting tuned trainingâ€¦")
train_result = reward_trainer.train()
print("âœ… train() finished. Global step:", train_result.global_step)
print("ðŸ“Š Final training metrics:", train_result.metrics)

# 9) Built-in evaluation
print("ðŸ” Running built-in evaluationâ€¦")
eval_metrics = reward_trainer.evaluate()
print("ðŸ·ï¸ Validation metrics:", eval_metrics)

# 10) Manual ranking accuracy
eval_data.set_format(
    type="torch",
    columns=[
        "input_ids_chosen", "attention_mask_chosen",
        "input_ids_rejected", "attention_mask_rejected"
    ]
)
eval_loader = DataLoader(eval_data, batch_size=8)
device = reward_model.device
reward_model.eval()

correct = total = 0
with torch.no_grad():
    for batch in eval_loader:
        sc = reward_model(
            input_ids=batch["input_ids_chosen"].to(device),
            attention_mask=batch["attention_mask_chosen"].to(device)
        ).logits.squeeze(-1)
        sr = reward_model(
            input_ids=batch["input_ids_rejected"].to(device),
            attention_mask=batch["attention_mask_rejected"].to(device)
        ).logits.squeeze(-1)
        correct += (sc > sr).sum().item()
        total += sc.size(0)

accuracy = correct / total
print(f"âœ… Tuned pairwise ranking accuracy = {accuracy:.4f}")

# 11) Save final model
trained = reward_trainer.model
trained.eval()
trained.config.num_labels = 1
trained.save_pretrained("/scratch/wt2244/reward_model_longformer0502_tuned2")
reward_tokenizer.save_pretrained("/scratch/wt2244/reward_model_longformer0502_tuned2")

print("ðŸŽ‰ Tuned reward model training complete and saved.")