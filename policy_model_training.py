# -*- coding: utf-8 -*-
"""policy_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5t3G3n1GUzD0qt9Jc8UlLXzDNgfqz64
"""

#!/usr/bin/env python
# 2025â€‘05â€‘04  â€“ PPO training (v9.3â€‘mod4â€‘resumeâ€‘5chk)
# â€¢ five checkpoints / epoch (20â€¯%,40â€¯%,60â€¯%,80â€¯%,100â€¯%)
# â€¢ autoâ€‘resume from the latest checkpoint (same logic as mod3)
# ---------------------------------------------------------------------

import os, re, math, torch, pandas as pd
from torch import nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,
    BitsAndBytesConfig, AutoConfig, LogitsProcessor, LogitsProcessorList
)
from peft import (
    PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training
)

torch.set_float32_matmul_precision("high")     # TF32 tensorâ€‘core

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
reward_dir   = "/scratch/wt2244/reward_model_longformer0502_tuned2"
policy_ckpt  = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
dataset_csv  = "/scratch/wt2244/final_feedback20250430.csv"
offload_dir  = "/scratch/wt2244/offload_ref"
out_dir      = "/scratch/wt2244/ppo_policy_lora05041841"
ckpt_dir     = "/scratch/wt2244/fivecheck1841"
os.makedirs(ckpt_dir, exist_ok=True)

assert torch.cuda.device_count() >= 3, "Need at least 3 GPUs!"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def print_gpu(tag=""):
    if tag:
        print(f"\n[GPU MEM] {tag}")
    for i in range(torch.cuda.device_count()):
        alloc = torch.cuda.memory_allocated(i) / 1024**2
        reserv = torch.cuda.memory_reserved(i) / 1024**2
        print(f" GPU{i}: alloc {alloc:8.1f}â€¯MB | reserved {reserv:8.1f}â€¯MB")
    print("-" * 60)

class BadLogitsProcessor(LogitsProcessor):
    def __init__(self):
        super().__init__()
        self.bad_batches = 0
    def __call__(self, _, scores):
        mask = torch.isnan(scores) | torch.isinf(scores)
        if mask.any():
            self.bad_batches += 1
            if self.bad_batches <= 3:
                print(f"[logits_fix] {mask.sum().item()} NaN/Inf â†’ -1e4")
            scores = scores.masked_fill(mask, -1e4)
        return scores

fault_lp = LogitsProcessorList([BadLogitsProcessor()])

def read_csv_robust(path, encodings=("utfâ€‘8", "latin1", "utfâ€‘16", "cp1252")):
    for enc in encodings:
        try:
            print(f"[dataset] trying '{enc}' â€¦")
            return pd.read_csv(path, encoding=enc)
        except UnicodeDecodeError as e:
            print(f"[dataset] âœ— '{enc}': {e}")
    raise UnicodeDecodeError("all encodings failed")

# â•â•â•â•â•â•â•â•â• reward â†’ GPU0 â•â•â•â•â•â•â•â•â•
reward_tok = AutoTokenizer.from_pretrained(reward_dir)
reward_tok.pad_token_id = reward_tok.pad_token_id or reward_tok.eos_token_id
cfg = AutoConfig.from_pretrained(reward_dir)
cfg.vocab_size, cfg.max_position_embeddings, cfg.type_vocab_size, cfg.num_labels = \
    len(reward_tok), 4098, 1, 1
reward_model = (
    AutoModelForSequenceClassification
    .from_pretrained(
        reward_dir,
        config=cfg,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=False,
        ignore_mismatched_sizes=True,
    )
    .to("cuda:0")
    .half()
    .eval()
)
reward_model.resize_token_embeddings(len(reward_tok))
print_gpu("after reward")

# â•â•â•â•â•â•â•â•â• policy (GPU0â€‘1, 4â€‘bit) â•â•â•â•â•â•â•â•â•
bnb = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)
print("Loading 14â€¯B policy â€¦")
policy = AutoModelForCausalLM.from_pretrained(
    policy_ckpt,
    quantization_config=bnb,
    device_map="auto",
    max_memory={0: "40GiB", 1: "40GiB", 2: "0GiB"},
)
policy = prepare_model_for_kbit_training(policy, use_gradient_checkpointing=True)

if os.path.isfile(os.path.join(out_dir, "adapter_config.json")):
    print("ğŸ”„  load LoRA adapter")
    policy = PeftModel.from_pretrained(policy, out_dir, is_trainable=True)
else:
    print("ğŸ†•  new LoRA adapter")
    policy = get_peft_model(
        policy,
        LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            target_modules=["q_proj", "v_proj"],
        ),
    )
policy.config.use_cache = False
policy.gradient_checkpointing_enable()
policy.train()

print_gpu("after policy")

tok_policy = AutoTokenizer.from_pretrained(policy_ckpt, use_fast=False)
tok_policy.pad_token_id = tok_policy.pad_token_id or tok_policy.eos_token_id

# â•â•â•â•â•â•â•â•â• reference â†’ GPU2 + offload â•â•â•â•â•â•â•â•â•
os.makedirs(offload_dir, exist_ok=True)
ref = AutoModelForCausalLM.from_pretrained(
    policy_ckpt,
    quantization_config=bnb,
    device_map="auto",
    max_memory={2: "70GiB", "cpu": "32GiB"},
    offload_folder=offload_dir,
).eval()
print_gpu("after reference")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ optimizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
opt = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, policy.parameters()),
    lr=2e-7,
    weight_decay=0.01,
    betas=(0.9, 0.95),
    eps=1e-8,
)

# â•â•â•â•â•â•â•â•â• dataset â•â•â•â•â•â•â•â•â•
df = read_csv_robust(dataset_csv)
prompt_col = next((c for c in ("prompt", "query") if c in df.columns), df.columns[0])
prompts = df[prompt_col].astype(str).tolist()
print(f"[dataset] using column '{prompt_col}', total {len(prompts)} rows")

# â•â•â•â•â•â•â•â•â• hyperâ€‘params â•â•â•â•â•â•â•â•â•
batch_size = 2
total_epochs = 4
initial_kl_clip = 5.0
final_kl_clip = 3.0
target_kl = 1.5
clip_eps = 0.2
gen_max_new_tokens = 400
kl_coef = 1.0
adapt_factor = 1.1

def log_probs_from_logits(logits: torch.Tensor, target_ids: torch.Tensor):
    logits = logits.float().clamp(-50, 50)
    log_z = torch.logsumexp(logits, -1, keepdim=True)
    log_p = logits - log_z
    return log_p.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)

num_batches = math.ceil(len(prompts) / batch_size)

# â”€â”€â”€ RESUME LOGIC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
resume_epoch = 0       # 0â€‘based
resume_batch = 0       # 1 â€¦ num_batches
step_idx = 0
ckpt_pattern = re.compile(r"ep(\d+)_b(\d+)\.pt")

def latest_ckpt(path: str):
    best_name = None
    best_ep = best_bt = -1
    for f in os.listdir(path):
        m = ckpt_pattern.fullmatch(f)
        if m:
            ep, bt = map(int, m.groups())
            if (ep, bt) > (best_ep, best_bt):
                best_ep, best_bt, best_name = ep, bt, f
    return best_name, best_ep, best_bt

ckpt_file, ckpt_ep, ckpt_bt = latest_ckpt(ckpt_dir)
if ckpt_file:
    full_path = os.path.join(ckpt_dir, ckpt_file)
    print(f"ğŸ”  Resuming from {full_path}")
    checkpoint = torch.load(full_path, map_location="cuda:0")
    state_dict = checkpoint["model_state_dict"]

    # Step 1: Strip _orig_mod. prefixes
    fixed_sd = {}
    for k, v in state_dict.items():
        if k.startswith("module._orig_mod."):
            new_k = k[len("module._orig_mod."):]
        elif k.startswith("_orig_mod."):
            new_k = k[len("_orig_mod."):]
        else:
            new_k = k
        fixed_sd[new_k] = v

    # Step 2: Remove BitsAndBytes quant keys
    quant_suffixes = (
        ".absmax", ".quant_map",
        ".nested_absmax", ".nested_quant_map",
        ".quant_state.bitsandbytes__nf4",
    )
    cleaned_sd = {k: v for k, v in fixed_sd.items()
                  if not k.endswith(quant_suffixes)}

    # Load into policy
    if hasattr(policy, "_orig_mod"):
        policy._orig_mod.load_state_dict(cleaned_sd, strict=False)
    else:
        policy.load_state_dict(cleaned_sd, strict=False)

    # Recompile & load optimizer
    policy = torch.compile(policy)
    opt.load_state_dict(checkpoint["optimizer_state_dict"])

    # â”€â”€ Align optimizer state with each parameterâ€™s device â”€â”€
    for group in opt.param_groups:
        for p in group["params"]:
            if p in opt.state:
                st = opt.state[p]
                for k, v in list(st.items()):
                    if isinstance(v, torch.Tensor):
                        st[k] = v.to(p.device)

    kl_coef     = checkpoint["kl_coef"]
    resume_epoch = checkpoint["epoch"] - 1
    resume_batch = checkpoint["batch"]
    step_idx     = checkpoint.get("step_idx",
                                  resume_epoch * num_batches + resume_batch)

    print(
        f"[resume] epoch {resume_epoch+1}, batch {resume_batch}, "
        f"step_idx {step_idx}, kl_coef {kl_coef:.3f}"
    )
else:
    print("ğŸš€  No checkpoint found â€“ starting from scratch")

# â”€â”€â”€ training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for ep in range(total_epochs):
    if ep < resume_epoch:
        continue

    # compute 5 save points (20â€‘%,â€¯40â€‘%,â€¯60â€‘%,â€¯80â€‘%,â€¯100â€¯%)
    save_points = {
        max(1, int(round(num_batches * frac)))
        for frac in (0.2, 0.4, 0.6, 0.8)
    }
    save_points.add(num_batches)

    print(f"\n=== Epoch {ep+1}/{total_epochs} ===")
    epoch_batch_idx = 0
    skip_count = 0

    for off in range(0, len(prompts), batch_size):
        epoch_batch_idx += 1
        if len(prompts) - off < batch_size:
            break

        if ep == resume_epoch and epoch_batch_idx <= resume_batch:
            continue

        step_idx += 1
        progress = epoch_batch_idx / num_batches
        kl_clip = initial_kl_clip - (initial_kl_clip - final_kl_clip) * progress

        # â”€â”€â”€ encode & generate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        batch_prompts = prompts[off : off + batch_size]
        enc_in = tok_policy(
            batch_prompts, return_tensors="pt",
            padding=True, truncation=True
        ).to("cuda:0")
        prompt_len = enc_in["attention_mask"].sum(1)

        policy.eval()
        with torch.no_grad():
            gen_out = policy.generate(
                **enc_in,
                max_new_tokens=gen_max_new_tokens,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                pad_token_id=tok_policy.eos_token_id,
                logits_processor=fault_lp,
                return_dict_in_generate=True,
                output_scores=True,
            )
        policy.train()
        print_gpu("after generate")

        seq_full = gen_out.sequences
        gen_text = [
            tok_policy.decode(s[p:], skip_special_tokens=True)
            for s, p in zip(seq_full, prompt_len)
        ]
        combos = [p + "\n" + g for p, g in zip(batch_prompts, gen_text)]

        # â”€â”€â”€ reward & advantage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        reward = reward_model(
            **reward_tok(
                combos,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=1024,
            ).to("cuda:0")
        ).logits.squeeze(-1)
        adv_raw = (reward - reward.mean()) / (reward.std() + 1e-6)

        # â”€â”€â”€ compute logâ€‘probs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        pol_logits = policy(seq_full.to("cuda:0")).logits
        pol_lp = log_probs_from_logits(pol_logits, seq_full.to("cuda:0"))
        logp_pol = torch.stack(
            [lp[p:].sum() for lp, p in zip(pol_lp, prompt_len)]
        )

        with torch.no_grad():
            ref_logits = ref(seq_full.to("cuda:2")).logits.float()
            ref_lp     = log_probs_from_logits(ref_logits, seq_full.to("cuda:2"))
            logp_ref   = torch.stack(
                [lp[p:].sum() for lp, p in zip(ref_lp, prompt_len)]
            ).to("cuda:0")

        diff     = (logp_pol - logp_ref).clamp(-60, 60)
        ratio    = torch.exp(diff).clamp(1e-4, 1e4)
        ppo_term = -torch.mean(
            torch.min(
                ratio * adv_raw,
                torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * adv_raw,
            )
        )
        kl_term  = diff.square().mean()

        if kl_term > kl_clip:
            skip_count += 1
            print(f"[earlyâ€‘stop] KL={kl_term:.3f} > {kl_clip:.3f}, skip batch")
            torch.cuda.empty_cache()
            continue

        print(
            f"[debug] adv.mean={adv_raw.mean():.3f}, "
            f"ratio.mean={ratio.mean():.3f}, kl_coef={kl_coef:.3f}, "
            f"kl_clip={kl_clip:.3f}"
        )

        loss = ppo_term + kl_coef * kl_term
        if not torch.isfinite(loss):
            print("[skip] nonâ€‘finite loss")
            continue

        loss.backward()
        nn.utils.clip_grad_norm_(policy.parameters(), 0.5)
        opt.step()
        opt.zero_grad()
        torch.cuda.empty_cache()

        # â”€â”€ adaptive kl_coef â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if step_idx > 10:
            if kl_term.item() > target_kl * 1.5:
                kl_coef *= adapt_factor
            elif kl_term.item() < target_kl * 0.5:
                kl_coef /= adapt_factor
            kl_coef = min(max(kl_coef, 0.01), 10.0)

        bad_cnt = fault_lp[0].bad_batches
        print_gpu(f"batch {step_idx} (bad={bad_cnt})")
        print(
            f"  loss={loss.item():+.4f} | reward={reward.mean():+.3f} "
            f"| kl={kl_term.item():+.3f}"
        )

        # â”€â”€â”€ checkpoint save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if epoch_batch_idx in save_points:
            path = os.path.join(ckpt_dir, f"ep{ep+1}_b{epoch_batch_idx}.pt")
            torch.save(
                {
                    "epoch": ep + 1,
                    "batch": epoch_batch_idx,
                    "step_idx": step_idx,
                    "model_state_dict": policy.state_dict(),
                    "optimizer_state_dict": opt.state_dict(),
                    "kl_coef": kl_coef,
                },
                path,
            )
            print(f"[checkpoint] saved â†’ {path}")

    print(f"=== Finished epoch {ep+1}/{total_epochs} ===")
    print(
        f"[epoch summary] skipped {skip_count}/{epoch_batch_idx} batches "
        f"({100 * skip_count / epoch_batch_idx:.1f}%)"
    )

print(f"\nâœ… All {total_epochs} epochs done â€“ adapter in {out_dir}")
